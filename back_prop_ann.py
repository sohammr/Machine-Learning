# -*- coding: utf-8 -*-
"""Copy of Copy of Execution_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15MW8LSS2vO1RmVyaHpqHh5l8Oc3E-Cex
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import argparse
import math
import pickle
import warnings
from sklearn.decomposition import PCA
warnings.filterwarnings("ignore")

def main():  
  
  np.random.seed(1234)
  
  #Default variables, will be changed if supplied by user
  optimizer = 'adam'
  gamma = 0.9
  anneal = True
  
  parser=argparse.ArgumentParser()
  parser.add_argument('--lr')
  parser.add_argument('--momentum')
  parser.add_argument('--num_hidden')
  parser.add_argument('--sizes')
  parser.add_argument('--activation')
  parser.add_argument('--loss')
  parser.add_argument('--opt')
  parser.add_argument('--batch_size')
  parser.add_argument('--epochs')
  parser.add_argument('--anneal')
  parser.add_argument('--save_dir')
  parser.add_argument('--expt_dir')
  parser.add_argument('--train')
  parser.add_argument('--val')
  parser.add_argument('--test')
  parser.add_argument('--pretrain')
  parser.add_argument('--state')
  parser.add_argument('--testing')
  
  args=parser.parse_args()
  
  pretrain = args.pretrain
  state = args.state
  testing = args.testing
  
  n_hidden=int(args.num_hidden)
  listsizes=args.sizes.split(',',n_hidden)
  sizes=[int(x) for x in listsizes]
  if args.lr != None:
    alpha=float(args.lr)
  if args.epochs != None:
    n_epochs=int(args.epochs)
  activ=args.activation
  if args.loss == 'sq':
    loss_function = 'se'
  else:
    loss_function = args.loss
  optimizer=args.opt
  if args.batch_size != None:
    batch_size=int(args.batch_size)
  if args.momentum != None:
    gamma=float(args.momentum)
  if args.anneal == 'true':
    anneal = True
  elif args.anneal == 'false':
    anneal = False
  
  global save_dir
  save_dir=args.save_dir
  global expt_dir
  expt_dir=args.expt_dir
  
  train = pd.read_csv(args.train)
  test = pd.read_csv(args.test)
  valid = pd.read_csv(args.val)
  
  X_train = train.iloc[:,1:785]
  X_test = test.iloc[:,1:785]
  X_val = valid.iloc[:,1:785]
  global y_train
  y_train = train.iloc[:,785]
  global y_train_oh
  y_train_oh = np.array( [ OneHotIt(y) for y in y_train  ]).T
  global y_val
  y_val = valid.iloc[:,785]
  global y_val_oh
  y_val_oh = np.array( [ OneHotIt(y) for y in y_val  ]).T
  
  pca = PCA(n_components = 100)
  pca.fit(X_train)
  
  X_train = pd.DataFrame(pca.transform(X_train))
  X_test = pd.DataFrame(pca.transform(X_test))
  X_val = pd.DataFrame(pca.transform(X_val))
  X_train = (X_train - X_train.mean())/(X_train.std())
  X_test = (X_test - X_train.mean())/(X_train.std())
  X_val = (X_val - X_train.mean())/(X_train.std())
  
  global h0
  h0=X_train.T
  global h0_val
  h0_val = X_val.T
  
  if pretrain == 'True':
    list_of_weights = load_weights(state, save_dir)
    if n_hidden == 1:
      w1,b1,w2,b2 = unpack_weights(list_of_weights)
    if n_hidden == 2:
      w1,b1,w2,b2,w3,b3 = unpack_weights(list_of_weights)
    if n_hidden == 3:
      w1,b1,w2,b2,w3,b3,w4,b4 = unpack_weights(list_of_weights)
    if n_hidden == 4:
      w1,b1,w2,b2,w3,b3,w4,b4,w5,b5 = unpack_weights(list_of_weights)
    if testing == 'False':
      if n_hidden == 1:
        w1,b1,w2,b2,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma, False, list_of_weights)
        list_of_weights_ = [w1,b1,w2,b2]
      elif n_hidden == 2:
        w1,b1,w2,b2,w3,b3,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma, False, list_of_weights)
        list_of_weights_ = [w1,b1,w2,b2,w3,b3]
      elif n_hidden == 3:
        w1,b1,w2,b2,w3,b3,w4,b4,log_list =ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma, False, list_of_weights)
        list_of_weights_ = [w1,b1,w2,b2,w3,b3,w4,b4]
      elif n_hidden == 4:
        w1,b1,w2,b2,w3,b3,w4,b4,w5,b5,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma, False, list_of_weights)
        list_of_weights_ = [w1,b1,w2,b2,w3,b3,w4,b4,w5,b5]
        
      #Saving log file
      with open(expt_dir+'log_train.txt', 'w') as f:
        for item in log_list:
            f.write('Epoch {}, '.format(item[0]) + 'Step {}, '.format(item[1]) +'Loss: {:.4f}, '.format(item[2]) +'Error: {:.2f},'.format(item[4]) +'lr: {}\n'.format(item[5]) )

      with open(expt_dir+'log_val.txt', 'w') as f:
        for item in log_list:
            f.write('Epoch {}, '.format(item[0]) + 'Step {}, '.format(item[1]) +'Loss: {:.4f}, '.format(item[3]) +'Error: {:.2f}, '.format(item[4]) +'lr: {}\n'.format(item[5]) )
        
   
  else:  
    if n_hidden == 1:
      w1,b1,w2,b2,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma)
      list_of_weights_ = [w1,b1,w2,b2]
    elif n_hidden == 2:
      w1,b1,w2,b2,w3,b3,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma)
      list_of_weights_ = [w1,b1,w2,b2,w3,b3]
    elif n_hidden == 3:
      w1,b1,w2,b2,w3,b3,w4,b4,log_list =ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma)
      list_of_weights_ = [w1,b1,w2,b2,w3,b3,w4,b4]
    elif n_hidden == 4:
      w1,b1,w2,b2,w3,b3,w4,b4,w5,b5,log_list = ann(n_hidden, sizes , alpha, n_epochs, activ, loss_function, anneal , batch_size , optimizer , gamma)
      list_of_weights_ = [w1,b1,w2,b2,w3,b3,w4,b4,w5,b5]

    #Saving log file
    with open(expt_dir+'log_train.txt', 'w') as f:
      for item in log_list:
          f.write('Epoch {}, '.format(item[0]) + 'Step {}, '.format(item[1]) +'Loss: {:.4f}, '.format(item[2]) +'Error: {:.2f},'.format(item[4]) +'lr: {}\n'.format(item[5]) )

    with open(expt_dir+'log_val.txt', 'w') as f:
      for item in log_list:
          f.write('Epoch {}, '.format(item[0]) + 'Step {}, '.format(item[1]) +'Loss: {:.4f}, '.format(item[3]) +'Error: {:.2f}, '.format(item[4]) +'lr: {}\n'.format(item[5]) )
        
        
  #Writing the csv file for predictions on test set
  h0_test = X_test.T
  activation = activation0(activ)
  if n_hidden == 1:
    a_1_t,a_2_t,h1_t,y_hat_test = forward_prop(n_hidden,activation,h0_test,w1,w2,b1,b2)
  elif n_hidden == 2:
    a_1_t,a_2_t,a_3_t,h1_t,h2_t,y_hat_test = forward_prop(n_hidden,activation,h0_test,w1,w2,w3,b1,b2,b3)
  elif n_hidden == 3:
    a_1_t,a_2_t,a_3_t,a_4_t,h1_t,h2_t,h3_t,y_hat_test = forward_prop(n_hidden,activation, h0_test,w1,w2,w3,w4,b1,b2,b3,b4)
  elif n_hidden == 4:
    a_1_t,a_2_t,a_3_t,a_4_t,a_5_t,h1_t,h2_t,h3_t,h4_t,y_hat_test = forward_prop(n_hidden,activation, h0_test,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
    
  y_pred_test = np.array([ (np.argmax(y)) for y in y_hat_test.T] )
  y_pred_test = pd.DataFrame({'id': range(len(y_pred_test)) , 'label': y_pred_test})
  y_pred_test.to_csv(expt_dir+'prediction_{}.csv'.format(state), index = False)

def save_weights(list_of_weights, epoch, save_dir):
      with open(save_dir +'weights_{}.pkl'.format(epoch), 'wb') as f:
             pickle.dump(list_of_weights, f)

def load_weights(state, save_dir):
      with open(save_dir +'weights_{}.pkl'.format(state), 'rb') as f:
              list_of_weights = pickle.load(f)
      return list_of_weights
    
def unpack_weights(list_of_weights_loaded):
  
  if len(list_of_weights_loaded) == 4 or len(list_of_weights_loaded) == 5 :
    w1 = list_of_weights_loaded[0]
    w2 = list_of_weights_loaded[1]
    b1 = list_of_weights_loaded[2]
    b2 = list_of_weights_loaded[3]
    return w1,b1,w2,b2
  
  if len(list_of_weights_loaded) == 6 or len(list_of_weights_loaded) == 7:
    w1 = list_of_weights_loaded[0]
    w2 = list_of_weights_loaded[1]
    w3 = list_of_weights_loaded[2]
    b1 = list_of_weights_loaded[3]
    b2 = list_of_weights_loaded[4]
    b3 = list_of_weights_loaded[5]
    return w1,b1,w2,b2,w3,b3
  
  if len(list_of_weights_loaded) == 8 or len(list_of_weights_loaded) == 9:
    w1 = list_of_weights_loaded[0]
    w2 = list_of_weights_loaded[1]
    w3 = list_of_weights_loaded[2]
    w4 = list_of_weights_loaded[3]
    b1 = list_of_weights_loaded[4]
    b2 = list_of_weights_loaded[5]
    b3 = list_of_weights_loaded[6]
    b4 = list_of_weights_loaded[7]
    return w1,b1,w2,b2,w3,b3,w4,b4
  
  if len(list_of_weights_loaded) == 10 or len(list_of_weights_loaded) == 11:
    w1 = list_of_weights_loaded[0]
    w2 = list_of_weights_loaded[1]
    w3 = list_of_weights_loaded[2]
    w4 = list_of_weights_loaded[3]
    w5 = list_of_weights_loaded[4]
    b1 = list_of_weights_loaded[5]
    b2 = list_of_weights_loaded[6]
    b3 = list_of_weights_loaded[7]
    b4 = list_of_weights_loaded[8]
    b5 = list_of_weights_loaded[9]
    return w1,b1,w2,b2,w3,b3,w4,b4,w5,b5

def sigmoid(x):
  return 1/(1+ np.exp(-x))

def tan_h(x):
  return np.tanh(x)

def relu(x):
  return (x + abs(x))/2

def activation0(activ):
  if activ == 'sigmoid':
    return sigmoid
  elif activ == 'tanh':
    return tan_h
  elif activ == 'relu':
    return relu

def softmax(z):
  #Subtracting max value from each column
  z = z - np.tile(z.max(axis = 0),z.shape[0]).reshape(z.shape)
  #Creating a matrix of denominators 
  denominator = np.exp(z).sum(axis = 0)
  denominator_matrix = np.tile(denominator,z.shape[0]).reshape(z.shape)
  return np.exp(z)/denominator_matrix

def cross_entropy(y_train_oh, y_hat):
  #Takes 2 vectors
  p_l = (y_train_oh * y_hat).sum() 
  if p_l == 0:
    return -np.log(10**(-4))
  else:
    return -np.log(p_l)

def cross_entropy_loss(y_train_oh, y_hat):
  #Takes 2 matrices
  return sum(np.array(([cross_entropy(y_oh,y) for y_oh,y in zip(y_train_oh.T,y_hat.T)])))/(y_train_oh.shape[1])

def squared_error(y_train_oh, y_hat):
  #Takes 2 vectors
  return ((y_train_oh - y_hat)**2).sum()

def squared_error_loss(y_train_oh, y_hat):
  #Takes 2 matrices
  return sum(np.array(([squared_error(y_oh,y) for y_oh,y in zip(y_train_oh.T,y_hat.T)])))/(y_train_oh.shape[1])

def OneHotIt(y):
  y_new = np.zeros(10)
  y_new[y] = 1
  return y_new

def sample_accuracy(y_train_oh, y_hat):
  if np.argmax(y_hat) == np.argmax(y_train_oh):
    return 1
  else:
    return 0

def accuracy(y_train_oh, y_hat):
  return sum(np.array(([sample_accuracy(y_oh,y) for y_oh,y in zip(y_train_oh.T,y_hat.T)]))) / (y_train_oh.shape[1])

# h0: input (784 x #samples)
# h0_bias: (785 x #samples)
# w1: weights of input layer to first hidden layer (a 784xn1 matrix) 
# w1_bias: 785 x n1
# b1: bias of input layer (an array) ( n1 length array)
# n1: number of neurons in first hidden layer

def hidden_layer1(h0, w1, b1, activation):
  n0 = h0.shape[0]
  n1 = w1.shape[1]
  h0_bias = np.append(h0 , np.ones(h0.shape[1]).reshape(1,-1), axis = 0 )
  w1_bias = np.append(w1, b1.reshape(1,-1), axis = 0 )
  a_1 = np.matmul(w1_bias.T,h0_bias)
  h_1 = np.array([activation(a) for a in a_1])
  return a_1,h_1

# h1: input (n1 x #samples)
# w2: weights of input layer to first hidden layer (a n1xn2 matrix) 
# b2: bias of input layer (an array) ( n3 length array)
# n2: number of neurons in output layer = 10

def output_layer(h1,w2,b2):
  n1 = h1.shape[0]
  n2 = w2.shape[1]
  h1_bias = np.append(h1 , np.ones(h1.shape[1]).reshape(1,-1), axis = 0 )
  w2_bias = np.append(w2, b2.reshape(1,-1), axis = 0 )
  a_2 = np.matmul(w2_bias.T,h1_bias)
  y_hat = softmax(np.matmul(w2_bias.T,h1_bias))
  return a_2,y_hat

def forward_prop(n_hidden,activation,*arg):
  if n_hidden==1:
    h0,w1,w2,b1,b2 = arg
    a_1,h1 = hidden_layer1(h0, w1, b1, activation)
    a_2,y_hat = output_layer(h1, w2, b2)
    return a_1,a_2,h1,y_hat
    
  if n_hidden==2:
    h0,w1,w2,w3,b1,b2,b3 = arg
    a_1,h1 = hidden_layer1(h0, w1, b1, activation)
    a_2,h2 = hidden_layer1(h1, w2, b2, activation)
    a_3,y_hat = output_layer(h2, w3, b3)
    return a_1,a_2,a_3,h1,h2,y_hat
    
  if n_hidden==3:
    h0,w1,w2,w3,w4,b1,b2,b3,b4 = arg
    a_1,h1 = hidden_layer1(h0, w1, b1, activation)
    a_2,h2 = hidden_layer1(h1, w2, b2, activation)
    a_3,h3 = hidden_layer1(h2, w3, b3, activation)
    a_4,y_hat = output_layer(h3, w4, b4)
    return a_1,a_2,a_3,a_4,h1,h2,h3,y_hat
    
  if n_hidden==4:
    h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5 = arg
    a_1,h1 = hidden_layer1(h0, w1, b1, activation)
    a_2,h2 = hidden_layer1(h1, w2, b2, activation)
    a_3,h3 = hidden_layer1(h2, w3, b3, activation)
    a_4,h4 = hidden_layer1(h3, w4, b4, activation)
    a_5,y_hat = output_layer(h4, w5, b5)
    return a_1,a_2,a_3,a_4,a_5,h1,h2,h3,h4,y_hat

def grad_aL_se(y_hat, y_train_oh):
  #Takes 2 vectors
  A = np.diag(y_hat)
  B = np.matmul(y_hat[:,np.newaxis], y_hat[:,np.newaxis].T)
  return 2* ( np.matmul( (A-B), (y_hat[:,np.newaxis] - y_train_oh[:,np.newaxis]) ) ).reshape(y_hat.shape[0])

def grad_softmax(y_hat, y_train_oh, loss):
  if loss == 'ce':
    return y_hat - y_train_oh
  elif loss == 'se':
    return np.array([grad_aL_se(y,y_oh) for y,y_oh in zip((y_hat.T),y_train_oh.T)]).T

"""*grad_softmax* returns $$\Delta_{a_L} L(\theta)  $$"""

def grad_h_i(wi_plus1, grad_a_i_plus1):
  return np.matmul(wi_plus1, grad_a_i_plus1)

def grad_a_i(grad_h_i, a_i, activation):
  return grad_h_i * g_dash(a_i, activation)

def g_dash(a_i,activation):
  if activation == sigmoid:
    return np.array( [sigmoid(a)*(1- sigmoid(a)) for a in a_i  ]  )
  elif activation == tan_h:
    return np.array( [(1 - tan_h(a))**2 for a in a_i  ]  )
  elif activation == relu:
    return np.array([(a + abs(a))/(2*a) for a in a_i])

def grad_w_k(grad_a_k, h_k_minus1):
  return (np.matmul(grad_a_k, h_k_minus1.T).T)/ (grad_a_k.shape[1])

def grad_b_k(grad_a_k):
#   ones = np.ones(grad_a_k.shape[1])
#   return np.matmul(grad_a_k, ones)/(grad_a_k.shape[1])
  return np.mean(grad_a_k, axis = 1)

def compute_loss(epoch,loss_function, y_train_oh, y_hat, loss_old = 0):
  
  if loss_function == 'ce':
    loss_new = cross_entropy_loss(y_train_oh, y_hat)
  elif loss_function == 'se':
    loss_new = squared_error_loss(y_train_oh, y_hat)
  
  if epoch==0:
    delta_loss = 0
  else:
    delta_loss = loss_new - loss_old
    
  return loss_new, delta_loss

"""##Batch gradient descent"""

def back_prop_batch(log_list,epoch,n_hidden,alpha,batch_size,activation,loss,*arg):
  
  if n_hidden == 1:
    y_train_oh,h0,w1,w2,b1,b2 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):

      batch_indices = range(i*batch_size,(i+1)*batch_size)
  
      y_oh = y_train_oh[:,batch_indices].copy()
      h_0 = np.array(h0.iloc[:,batch_indices].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
      
      #Forward prop train
      a1,a2,h_1,y = forward_prop(n_hidden,activation, h_0,w1,w2,b1,b2)

      grad_a_2 = grad_softmax(y, y_oh, loss)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      w1 = w1 - alpha*grad_w_1
      b1 = b1 - alpha*grad_b_1
      w2 = w2 - alpha*grad_w_2
      b2 = b2 - alpha*grad_b_2
      
      
      if i%100 == 0:
        #Forward prop train
        a_1,a_2,h1,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,b1,b2)
        #Forward prop val
        a_1_val,a_2_val,h1_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,b1,b2)
        
        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
      
    return w1,w2,b1,b2
  
  
  if n_hidden == 2:
    y_train_oh,h0,w1,w2,w3,b1,b2,b3 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
          
      a1,a2,a3,h_1,h_2,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,b1,b2,b3)

      grad_a_3 = grad_softmax(y, y_oh, loss)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      w1 = w1 - alpha*grad_w_1
      b1 = b1 - alpha*grad_b_1
      w2 = w2 - alpha*grad_w_2
      b2 = b2 - alpha*grad_b_2
      w3 = w3 - alpha*grad_w_3
      b3 = b3 - alpha*grad_b_3
      
     
      if i%100 == 0:
         #Forward prop
        a_1,a_2,a_3,h1,h2,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,b1,b2,b3)   
        #Forward prop val
        a_1_val,a_2_val,a_3_val,h1_val,h2_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,b1,b2,b3)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,w2,w3,b1,b2,b3
  
  
  if n_hidden == 3:
    y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())
      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      a1,a2,a3,a4,h_1,h_2,h_3,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,b1,b2,b3,b4)

      grad_a_4 = grad_softmax(y, y_oh, loss)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      w1 = w1 - alpha*grad_w_1
      b1 = b1 - alpha*grad_b_1
      w2 = w2 - alpha*grad_w_2
      b2 = b2 - alpha*grad_b_2
      w3 = w3 - alpha*grad_w_3
      b3 = b3 - alpha*grad_b_3
      w4 = w4 - alpha*grad_w_4
      b4 = b4 - alpha*grad_b_4
      
      
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,h1,h2,h3,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,b1,b2,b3,b4)  
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,h1_val,h2_val,h3_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,b1,b2,b3,b4)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,w2,w3,w4,b1,b2,b3,b4
  
  
  if n_hidden == 4:
    y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      #Forward prop
      a1,a2,a3,a4,a5,h_1,h_2,h_3,h_4,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
      
      grad_a_5 = grad_softmax(y, y_oh, loss)
      grad_w_5 = grad_w_k(grad_a_5, h_4)
      grad_b_5 = grad_b_k(grad_a_5)
      grad_h_4 = grad_h_i(w5, grad_a_5)
      grad_a_4 = grad_a_i(grad_h_4, a4, activation)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      w1 = w1 - alpha*grad_w_1
      b1 = b1 - alpha*grad_b_1
      w2 = w2 - alpha*grad_w_2
      b2 = b2 - alpha*grad_b_2
      w3 = w3 - alpha*grad_w_3
      b3 = b3 - alpha*grad_b_3
      w4 = w4 - alpha*grad_w_4
      b4 = b4 - alpha*grad_b_4
      w5 = w5 - alpha*grad_w_5
      b5 = b5 - alpha*grad_b_5
      
     
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,a_5,h1,h2,h3,h4,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,a_5_val,h1_val,h2_val,h3_val,h4_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,w2,w3,w4,w5,b1,b2,b3,b4,b5

"""## Momentum based gradient descent"""

def momentum_back_prop_batch(log_list,epoch,n_hidden,alpha,batch_size,activation,loss,gamma,*arg):
  
  if n_hidden == 1:
    
    y_train_oh,h0,w1,w2,b1,b2,prev_v = arg 
    
    if epoch==0:      
      #Initialising momentum parameters
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      batch_indices = range(i*batch_size,(i+1)*batch_size)
     
      y_oh = y_train_oh[:,batch_indices].copy()
      h_0 = np.array(h0.iloc[:,batch_indices].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      #Forward prop train
      a1,a2,h_1,y = forward_prop(n_hidden,activation, h_0,w1,w2,b1,b2)
      
      grad_a_2 = grad_softmax(y, y_oh, loss)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
     
      #momentum update
      v_w1=gamma*prev_v_w1+alpha*grad_w_1
      v_b1=gamma*prev_v_b1+alpha*grad_b_1
      v_w2=gamma*prev_v_w2+alpha*grad_w_2
      v_b2=gamma*prev_v_b2+alpha*grad_b_2
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v = [prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2]
      
      
      if i%100 == 0:
        #Forward prop train
        a_1,a_2,h1,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,b1,b2)
        #Forward prop val
        a_1_val,a_2_val,h1_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,b1,b2)
        
        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])

    return w1,b1,w2,b2,prev_v
  
  
  if n_hidden == 2:
    y_train_oh,h0,w1,w2,w3,b1,b2,b3,prev_v = arg 
    
    if epoch==0:      
      #Initialising momentum parameters
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
          
      a1,a2,a3,h_1,h_2,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,b1,b2,b3)

      grad_a_3 = grad_softmax(y, y_oh, loss)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v_w1+alpha*grad_w_1
      v_b1=gamma*prev_v_b1+alpha*grad_b_1
      v_w2=gamma*prev_v_w2+alpha*grad_w_2
      v_b2=gamma*prev_v_b2+alpha*grad_b_2
      v_w3=gamma*prev_v_w3+alpha*grad_w_3
      v_b3=gamma*prev_v_b3+alpha*grad_b_3
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3]
     
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,h1,h2,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,b1,b2,b3)   
        #Forward prop val
        a_1_val,a_2_val,a_3_val,h1_val,h2_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,b1,b2,b3)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,b1,w2,b2,w3,b3,prev_v
  
  
  
  if n_hidden == 3:
    y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4,prev_v= arg 
    
    if epoch==0:      
      #Initialising momentum parameters
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
      prev_v_w4=np.zeros(w4.shape)
      prev_v_b4=np.zeros(b4.shape)
    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]
      prev_v_w4=prev_v[6]
      prev_v_b4=prev_v[7]
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())
      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      a1,a2,a3,a4,h_1,h_2,h_3,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,b1,b2,b3,b4)
      
      grad_a_4 = grad_softmax(y, y_oh, loss)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v_w1+alpha*grad_w_1
      v_b1=gamma*prev_v_b1+alpha*grad_b_1
      v_w2=gamma*prev_v_w2+alpha*grad_w_2
      v_b2=gamma*prev_v_b2+alpha*grad_b_2
      v_w3=gamma*prev_v_w3+alpha*grad_w_3
      v_b3=gamma*prev_v_b3+alpha*grad_b_3
      v_w4=gamma*prev_v_w4+alpha*grad_w_4
      v_b4=gamma*prev_v_b4+alpha*grad_b_4
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      w4=w4-v_w4
      b4=b4-v_b4
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      prev_v_w4=v_w4
      prev_v_b4=v_b4
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4]
      
      
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,h1,h2,h3,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,b1,b2,b3,b4)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,h1_val,h2_val,h3_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,b1,b2,b3,b4)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,b1,w2,b2,w3,b3,w4,b4,prev_v
  
  
  if n_hidden == 4:
    
    y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,prev_v= arg 
    
    if epoch==0:      
      #Initialising momentum parameters
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
      prev_v_w4=np.zeros(w4.shape)
      prev_v_b4=np.zeros(b4.shape)
      prev_v_w5=np.zeros(w5.shape)
      prev_v_b5=np.zeros(b5.shape)
    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]
      prev_v_w4=prev_v[6]
      prev_v_b4=prev_v[7]
      prev_v_w5=prev_v[8]
      prev_v_b5=prev_v[9]
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      #Forward prop
      a1,a2,a3,a4,a5,h_1,h_2,h_3,h_4,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
               
      grad_a_5 = grad_softmax(y, y_oh, loss)
      grad_w_5 = grad_w_k(grad_a_5, h_4)
      grad_b_5 = grad_b_k(grad_a_5)
      grad_h_4 = grad_h_i(w5, grad_a_5)
      grad_a_4 = grad_a_i(grad_h_4, a4, activation)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v_w1+alpha*grad_w_1
      v_b1=gamma*prev_v_b1+alpha*grad_b_1
      v_w2=gamma*prev_v_w2+alpha*grad_w_2
      v_b2=gamma*prev_v_b2+alpha*grad_b_2
      v_w3=gamma*prev_v_w3+alpha*grad_w_3
      v_b3=gamma*prev_v_b3+alpha*grad_b_3
      v_w4=gamma*prev_v_w4+alpha*grad_w_4
      v_b4=gamma*prev_v_b4+alpha*grad_b_4
      v_w5=gamma*prev_v_w5+alpha*grad_w_5
      v_b5=gamma*prev_v_b5+alpha*grad_b_5
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      w4=w4-v_w4
      b4=b4-v_b4
      w5=w5-v_w5
      b5=b5-v_b5
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      prev_v_w4=v_w4
      prev_v_b4=v_b4
      prev_v_w5=v_w5
      prev_v_b5=v_b5
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4,prev_v_w5,prev_v_b5]
      
      
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,a_5,h1,h2,h3,h4,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,a_5_val,h1_val,h2_val,h3_val,h4_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, alpha])
     
    return w1,b1,w2,b2,w3,b3,w4,b4,w5,b5,prev_v

"""## Adam"""

def back_prop_batch_ADAM(log_list,epoch,n_hidden,lr,batch_size,activation, loss, beta1,beta2,eps,m,v,*arg):
  
  if n_hidden == 1:
    y_train_oh,h0,w1,w2,b1,b2 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      m_w1=np.zeros(w1.shape)
      m_b1=np.zeros(b1.shape)
      m_w2=np.zeros(w2.shape)
      m_b2=np.zeros(b2.shape)

      v_w1=np.zeros(w1.shape)
      v_b1=np.zeros(b1.shape)
      v_w2=np.zeros(w2.shape)
      v_b2=np.zeros(b2.shape)

      m_w1_hat=np.zeros(w1.shape)
      m_b1_hat=np.zeros(b1.shape)
      m_w2_hat=np.zeros(w2.shape)
      m_b2_hat=np.zeros(b2.shape)

      v_w1_hat=np.zeros(w1.shape)
      v_b1_hat=np.zeros(b1.shape)
      v_w2_hat=np.zeros(w2.shape)
      v_b2_hat=np.zeros(b2.shape)

    else:
      m_w1=m[0]
      m_b1=m[1]
      m_w2=m[2]
      m_b2=m[3]
      
      v_w1=v[0]
      v_b1=v[1]
      v_w2=v[2]
      v_b2=v[3]
    
    for i in range(n_batches):

      batch_indices = range(i*batch_size,(i+1)*batch_size)         

      y_oh = y_train_oh[:,batch_indices].copy()
      h_0 = np.array(h0.iloc[:,batch_indices].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
      
      #Forward prop train
      a1,a2,h_1,y = forward_prop(n_hidden,activation, h_0,w1,w2,b1,b2)

      grad_a_2 = grad_softmax(y, y_oh, loss)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)

      m_w1=beta1*m_w1+(1-beta1)*grad_w_1
      m_b1=beta1*m_b1+(1-beta1)*grad_b_1
      m_w2=beta1*m_w2+(1-beta1)*grad_w_2
      m_b2=beta1*m_b2+(1-beta1)*grad_b_2

      m=[m_w1,m_b1,m_w2,m_b2]

      v_w1=beta2*v_w1+(1-beta2)*grad_w_1**2
      v_b1=beta2*v_b1+(1-beta2)*grad_b_1**2
      v_w2=beta2*v_w2+(1-beta2)*grad_w_2**2
      v_b2=beta2*v_b2+(1-beta2)*grad_b_2**2

      v=[v_w1,v_b1,v_w2,v_b2]

      m_w1_hat=m_w1/(1-math.pow(beta1,i+1))
      m_b1_hat=m_b1/(1-math.pow(beta1,i+1))
      m_w2_hat=m_w2/(1-math.pow(beta1,i+1))
      m_b2_hat=m_b2/(1-math.pow(beta1,i+1))

      v_w1_hat=v_w1/(1-math.pow(beta2,i+1))
      v_b1_hat=v_b1/(1-math.pow(beta2,i+1))
      v_w2_hat=v_w2/(1-math.pow(beta2,i+1))
      v_b2_hat=v_b2/(1-math.pow(beta2,i+1))

      w1=w1-(lr/np.sqrt(v_w1_hat+eps))*m_w1_hat
      b1=b1-(lr/np.sqrt(v_b1_hat+eps))*m_b1_hat
      w2=w2-(lr/np.sqrt(v_w2_hat+eps))*m_w2_hat
      b2=b2-(lr/np.sqrt(v_b2_hat+eps))*m_b2_hat
      
      if i%100 == 0:
        #Forward prop train
        a_1_,a_2_,h1_,y_hat_ = forward_prop(n_hidden,activation, h0,w1,w2,b1,b2)
        #Forward prop val
        a_1_val,a_2_val,h1_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,b1,b2)
        
        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat_)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat_)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])

    return w1,w2,b1,b2,m,v
  
  
  if n_hidden == 2:
    y_train_oh,h0,w1,w2,w3,b1,b2,b3= arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      m_w1=np.zeros(w1.shape)
      m_b1=np.zeros(b1.shape)
      m_w2=np.zeros(w2.shape)
      m_b2=np.zeros(b2.shape)
      m_w3=np.zeros(w3.shape)
      m_b3=np.zeros(b3.shape)


      v_w1=np.zeros(w1.shape)
      v_b1=np.zeros(b1.shape)
      v_w2=np.zeros(w2.shape)
      v_b2=np.zeros(b2.shape)
      v_w3=np.zeros(w3.shape)
      v_b3=np.zeros(b3.shape)

      m_w1_hat=np.zeros(w1.shape)
      m_b1_hat=np.zeros(b1.shape)
      m_w2_hat=np.zeros(w2.shape)
      m_b2_hat=np.zeros(b2.shape)
      m_w3_hat=np.zeros(w3.shape)
      m_b3_hat=np.zeros(b3.shape)

      v_w1_hat=np.zeros(w1.shape)
      v_b1_hat=np.zeros(b1.shape)
      v_w2_hat=np.zeros(w2.shape)
      v_b2_hat=np.zeros(b2.shape)
      v_w3_hat=np.zeros(w3.shape)
      v_b3_hat=np.zeros(b3.shape)
      
    else:
      m_w1=m[0]
      m_b1=m[1]
      m_w2=m[2]
      m_b2=m[3]
      m_w3=m[4]
      m_b3=m[5]
      
      v_w1=v[0]
      v_b1=v[1]
      v_w2=v[2]
      v_b2=v[3]
      v_w3=v[4]
      v_b3=v[5]
      
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
          
      a1,a2,a3,h_1,h_2,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,b1,b2,b3)

      grad_a_3 = grad_softmax(y, y_oh, loss)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      m_w1=beta1*m_w1+(1-beta1)*grad_w_1
      m_b1=beta1*m_b1+(1-beta1)*grad_b_1
      m_w2=beta1*m_w2+(1-beta1)*grad_w_2
      m_b2=beta1*m_b2+(1-beta1)*grad_b_2
      m_w3=beta1*m_w3+(1-beta1)*grad_w_3
      m_b3=beta1*m_b3+(1-beta1)*grad_b_3

      m=[m_w1,m_b1,m_w2,m_b2,m_w3,m_b3]

      v_w1=beta2*v_w1+(1-beta2)*grad_w_1**2
      v_b1=beta2*v_b1+(1-beta2)*grad_b_1**2
      v_w2=beta2*v_w2+(1-beta2)*grad_w_2**2
      v_b2=beta2*v_b2+(1-beta2)*grad_b_2**2
      v_w3=beta2*v_w3+(1-beta2)*grad_w_3**2
      v_b3=beta2*v_b3+(1-beta2)*grad_b_3**2

      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3]

      m_w1_hat=m_w1/(1-math.pow(beta1,i+1))
      m_b1_hat=m_b1/(1-math.pow(beta1,i+1))
      m_w2_hat=m_w2/(1-math.pow(beta1,i+1))
      m_b2_hat=m_b2/(1-math.pow(beta1,i+1))
      m_w3_hat=m_w3/(1-math.pow(beta1,i+1))
      m_b3_hat=m_b3/(1-math.pow(beta1,i+1))

      v_w1_hat=v_w1/(1-math.pow(beta2,i+1))
      v_b1_hat=v_b1/(1-math.pow(beta2,i+1))
      v_w2_hat=v_w2/(1-math.pow(beta2,i+1))
      v_b2_hat=v_b2/(1-math.pow(beta2,i+1))
      v_w3_hat=v_w3/(1-math.pow(beta2,i+1))
      v_b3_hat=v_b3/(1-math.pow(beta2,i+1))

      w1=w1-(lr/np.sqrt(v_w1_hat+eps))*m_w1_hat
      b1=b1-(lr/np.sqrt(v_b1_hat+eps))*m_b1_hat
      w2=w2-(lr/np.sqrt(v_w2_hat+eps))*m_w2_hat
      b2=b2-(lr/np.sqrt(v_b2_hat+eps))*m_b2_hat
      w3=w3-(lr/np.sqrt(v_w3_hat+eps))*m_w3_hat
      b3=b3-(lr/np.sqrt(v_b3_hat+eps))*m_b3_hat
      
      if i%100 == 0:
        
        #Forward prop
        a_1_,a_2_,a_3_,h1_,h2_,y_hat_ = forward_prop(n_hidden,activation, h0,w1,w2,w3,b1,b2,b3)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,h1_val,h2_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,b1,b2,b3)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat_)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat_)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      

    return w1,w2,w3,b1,b2,b3,m,v
  
  if n_hidden == 3:
    y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4= arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      m_w1=np.zeros(w1.shape)
      m_b1=np.zeros(b1.shape)
      m_w2=np.zeros(w2.shape)
      m_b2=np.zeros(b2.shape)
      m_w3=np.zeros(w3.shape)
      m_b3=np.zeros(b3.shape)
      m_w4=np.zeros(w4.shape)
      m_b4=np.zeros(b4.shape)


      v_w1=np.zeros(w1.shape)
      v_b1=np.zeros(b1.shape)
      v_w2=np.zeros(w2.shape)
      v_b2=np.zeros(b2.shape)
      v_w3=np.zeros(w3.shape)
      v_b3=np.zeros(b3.shape)
      v_w4=np.zeros(w4.shape)
      v_b4=np.zeros(b4.shape)

      m_w1_hat=np.zeros(w1.shape)
      m_b1_hat=np.zeros(b1.shape)
      m_w2_hat=np.zeros(w2.shape)
      m_b2_hat=np.zeros(b2.shape)
      m_w3_hat=np.zeros(w3.shape)
      m_b3_hat=np.zeros(b3.shape)
      m_w4_hat=np.zeros(w4.shape)
      m_b4_hat=np.zeros(b4.shape)

      v_w1_hat=np.zeros(w1.shape)
      v_b1_hat=np.zeros(b1.shape)
      v_w2_hat=np.zeros(w2.shape)
      v_b2_hat=np.zeros(b2.shape)
      v_w3_hat=np.zeros(w3.shape)
      v_b3_hat=np.zeros(b3.shape)
      v_w4_hat=np.zeros(w4.shape)
      v_b4_hat=np.zeros(b4.shape)
      
    else:
      m_w1=m[0]
      m_b1=m[1]
      m_w2=m[2]
      m_b2=m[3]
      m_w3=m[4]
      m_b3=m[5]
      m_w4=m[6]
      m_b4=m[7]
      
      v_w1=v[0]
      v_b1=v[1]
      v_w2=v[2]
      v_b2=v[3]
      v_w3=v[4]
      v_b3=v[5]
      v_w4=v[6]
      v_b4=v[7]
    
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())
      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      a1,a2,a3,a4,h_1,h_2,h_3,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,b1,b2,b3,b4)
      
      grad_a_4 = grad_softmax(y, y_oh, loss)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      m_w1=beta1*m_w1+(1-beta1)*grad_w_1
      m_b1=beta1*m_b1+(1-beta1)*grad_b_1
      m_w2=beta1*m_w2+(1-beta1)*grad_w_2
      m_b2=beta1*m_b2+(1-beta1)*grad_b_2
      m_w3=beta1*m_w3+(1-beta1)*grad_w_3
      m_b3=beta1*m_b3+(1-beta1)*grad_b_3
      m_w4=beta1*m_w4+(1-beta1)*grad_w_4
      m_b4=beta1*m_b4+(1-beta1)*grad_b_4

      m=[m_w1,m_b1,m_w2,m_b2,m_w3,m_b3,m_w4,m_b4]

      v_w1=beta2*v_w1+(1-beta2)*grad_w_1**2
      v_b1=beta2*v_b1+(1-beta2)*grad_b_1**2
      v_w2=beta2*v_w2+(1-beta2)*grad_w_2**2
      v_b2=beta2*v_b2+(1-beta2)*grad_b_2**2
      v_w3=beta2*v_w3+(1-beta2)*grad_w_3**2
      v_b3=beta2*v_b3+(1-beta2)*grad_b_3**2
      v_w4=beta2*v_w4+(1-beta2)*grad_w_4**2
      v_b4=beta2*v_b4+(1-beta2)*grad_b_4**2


      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4]

      m_w1_hat=m_w1/(1-math.pow(beta1,i+1))
      m_b1_hat=m_b1/(1-math.pow(beta1,i+1))
      m_w2_hat=m_w2/(1-math.pow(beta1,i+1))
      m_b2_hat=m_b2/(1-math.pow(beta1,i+1))
      m_w3_hat=m_w3/(1-math.pow(beta1,i+1))
      m_b3_hat=m_b3/(1-math.pow(beta1,i+1))
      m_w4_hat=m_w4/(1-math.pow(beta1,i+1))
      m_b4_hat=m_b4/(1-math.pow(beta1,i+1))

      v_w1_hat=v_w1/(1-math.pow(beta2,i+1))
      v_b1_hat=v_b1/(1-math.pow(beta2,i+1))
      v_w2_hat=v_w2/(1-math.pow(beta2,i+1))
      v_b2_hat=v_b2/(1-math.pow(beta2,i+1))
      v_w3_hat=v_w3/(1-math.pow(beta2,i+1))
      v_b3_hat=v_b3/(1-math.pow(beta2,i+1))
      v_w4_hat=v_w4/(1-math.pow(beta2,i+1))
      v_b4_hat=v_b4/(1-math.pow(beta2,i+1))

      w1=w1-(lr/np.sqrt(v_w1_hat+eps))*m_w1_hat
      b1=b1-(lr/np.sqrt(v_b1_hat+eps))*m_b1_hat
      w2=w2-(lr/np.sqrt(v_w2_hat+eps))*m_w2_hat
      b2=b2-(lr/np.sqrt(v_b2_hat+eps))*m_b2_hat
      w3=w3-(lr/np.sqrt(v_w3_hat+eps))*m_w3_hat
      b3=b3-(lr/np.sqrt(v_b3_hat+eps))*m_b3_hat
      w4=w4-(lr/np.sqrt(v_w4_hat+eps))*m_w4_hat
      b4=b4-(lr/np.sqrt(v_b4_hat+eps))*m_b4_hat
      
      if i%100 == 0:
        
        #Forward prop
        a_1_,a_2_,a_3_,a_4_,h1_,h2_,h3_,y_hat_ = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,b1,b2,b3,b4)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,h1_val,h2_val,h3_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,b1,b2,b3,b4)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat_)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat_)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
    return w1,w2,w3,w4,b1,b2,b3,b4,m,v
      
  
  
  if n_hidden == 4:
    
    y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5= arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      m_w1=np.zeros(w1.shape)
      m_b1=np.zeros(b1.shape)
      m_w2=np.zeros(w2.shape)
      m_b2=np.zeros(b2.shape)
      m_w3=np.zeros(w3.shape)
      m_b3=np.zeros(b3.shape)
      m_w4=np.zeros(w4.shape)
      m_b4=np.zeros(b4.shape)
      m_w5=np.zeros(w5.shape)
      m_b5=np.zeros(b5.shape)


      v_w1=np.zeros(w1.shape)
      v_b1=np.zeros(b1.shape)
      v_w2=np.zeros(w2.shape)
      v_b2=np.zeros(b2.shape)
      v_w3=np.zeros(w3.shape)
      v_b3=np.zeros(b3.shape)
      v_w4=np.zeros(w4.shape)
      v_b4=np.zeros(b4.shape)
      v_w5=np.zeros(w5.shape)
      v_b5=np.zeros(b5.shape)


      m_w1_hat=np.zeros(w1.shape)
      m_b1_hat=np.zeros(b1.shape)
      m_w2_hat=np.zeros(w2.shape)
      m_b2_hat=np.zeros(b2.shape)
      m_w3_hat=np.zeros(w3.shape)
      m_b3_hat=np.zeros(b3.shape)
      m_w4_hat=np.zeros(w4.shape)
      m_b4_hat=np.zeros(b4.shape)
      m_w5_hat=np.zeros(w5.shape)
      m_b5_hat=np.zeros(b5.shape)

      v_w1_hat=np.zeros(w1.shape)
      v_b1_hat=np.zeros(b1.shape)
      v_w2_hat=np.zeros(w2.shape)
      v_b2_hat=np.zeros(b2.shape)
      v_w3_hat=np.zeros(w3.shape)
      v_b3_hat=np.zeros(b3.shape)
      v_w4_hat=np.zeros(w4.shape)
      v_b4_hat=np.zeros(b4.shape)
      v_w5_hat=np.zeros(w5.shape)
      v_b5_hat=np.zeros(b5.shape)
      
    else:
      m_w1=m[0]
      m_b1=m[1]
      m_w2=m[2]
      m_b2=m[3]
      m_w3=m[4]
      m_b3=m[5]
      m_w4=m[6]
      m_b4=m[7]
      m_w5=m[8]
      m_b5=m[9]
      
      v_w1=v[0]
      v_b1=v[1]
      v_w2=v[2]
      v_b2=v[3]
      v_w3=v[4]
      v_b3=v[5]
      v_w4=v[6]
      v_b4=v[7]
      v_w5=v[8]
      v_b5=v[9]
      
    for i in range(n_batches):
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      #Forward prop
      a1,a2,a3,a4,a5,h_1,h_2,h_3,h_4,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
          
      grad_a_5 = grad_softmax(y, y_oh, loss)
      grad_w_5 = grad_w_k(grad_a_5, h_4)
      grad_b_5 = grad_b_k(grad_a_5)
      grad_h_4 = grad_h_i(w5, grad_a_5)
      grad_a_4 = grad_a_i(grad_h_4, a4, activation)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4, grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3, activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3, grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2, activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1, activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      m_w1=beta1*m_w1+(1-beta1)*grad_w_1
      m_b1=beta1*m_b1+(1-beta1)*grad_b_1
      m_w2=beta1*m_w2+(1-beta1)*grad_w_2
      m_b2=beta1*m_b2+(1-beta1)*grad_b_2
      m_w3=beta1*m_w3+(1-beta1)*grad_w_3
      m_b3=beta1*m_b3+(1-beta1)*grad_b_3
      m_w4=beta1*m_w4+(1-beta1)*grad_w_4
      m_b4=beta1*m_b4+(1-beta1)*grad_b_4
      m_w5=beta1*m_w5+(1-beta1)*grad_w_5
      m_b5=beta1*m_b5+(1-beta1)*grad_b_5

      m=[m_w1,m_b1,m_w2,m_b2,m_w3,m_b3,m_w4,m_b4,m_w5,m_b5]

      v_w1=beta2*v_w1+(1-beta2)*grad_w_1**2
      v_b1=beta2*v_b1+(1-beta2)*grad_b_1**2
      v_w2=beta2*v_w2+(1-beta2)*grad_w_2**2
      v_b2=beta2*v_b2+(1-beta2)*grad_b_2**2
      v_w3=beta2*v_w3+(1-beta2)*grad_w_3**2
      v_b3=beta2*v_b3+(1-beta2)*grad_b_3**2
      v_w4=beta2*v_w4+(1-beta2)*grad_w_4**2
      v_b4=beta2*v_b4+(1-beta2)*grad_b_4**2
      v_w5=beta2*v_w5+(1-beta2)*grad_w_5**2
      v_b5=beta2*v_b5+(1-beta2)*grad_b_5**2


      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4,v_w5,v_b5]

      m_w1_hat=m_w1/(1-math.pow(beta1,i+1))
      m_b1_hat=m_b1/(1-math.pow(beta1,i+1))
      m_w2_hat=m_w2/(1-math.pow(beta1,i+1))
      m_b2_hat=m_b2/(1-math.pow(beta1,i+1))
      m_w3_hat=m_w3/(1-math.pow(beta1,i+1))
      m_b3_hat=m_b3/(1-math.pow(beta1,i+1))
      m_w4_hat=m_w4/(1-math.pow(beta1,i+1))
      m_b4_hat=m_b4/(1-math.pow(beta1,i+1))
      m_w5_hat=m_w5/(1-math.pow(beta1,i+1))
      m_b5_hat=m_b5/(1-math.pow(beta1,i+1))

      v_w1_hat=v_w1/(1-math.pow(beta2,i+1))
      v_b1_hat=v_b1/(1-math.pow(beta2,i+1))
      v_w2_hat=v_w2/(1-math.pow(beta2,i+1))
      v_b2_hat=v_b2/(1-math.pow(beta2,i+1))
      v_w3_hat=v_w3/(1-math.pow(beta2,i+1))
      v_b3_hat=v_b3/(1-math.pow(beta2,i+1))
      v_w4_hat=v_w4/(1-math.pow(beta2,i+1))
      v_b4_hat=v_b4/(1-math.pow(beta2,i+1))
      v_w5_hat=v_w5/(1-math.pow(beta2,i+1))
      v_b5_hat=v_b5/(1-math.pow(beta2,i+1))

      w1=w1-(lr/np.sqrt(v_w1_hat+eps))*m_w1_hat
      b1=b1-(lr/np.sqrt(v_b1_hat+eps))*m_b1_hat
      w2=w2-(lr/np.sqrt(v_w2_hat+eps))*m_w2_hat
      b2=b2-(lr/np.sqrt(v_b2_hat+eps))*m_b2_hat
      w3=w3-(lr/np.sqrt(v_w3_hat+eps))*m_w3_hat
      b3=b3-(lr/np.sqrt(v_b3_hat+eps))*m_b3_hat
      w4=w4-(lr/np.sqrt(v_w4_hat+eps))*m_w4_hat
      b4=b4-(lr/np.sqrt(v_b4_hat+eps))*m_b4_hat
      w5=w5-(lr/np.sqrt(v_w5_hat+eps))*m_w5_hat
      b5=b5-(lr/np.sqrt(v_b5_hat+eps))*m_b5_hat
      
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,a_5,h1,h2,h3,h4,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,a_5_val,h1_val,h2_val,h3_val,h4_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
    return w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,m,v
  
  '''#NAG'''

def back_prop_batch_NAG(log_list,epoch,n_hidden,lr,batch_size,activation,loss,gamma,v,prev_v,*arg):
  
  if n_hidden == 1:
    y_train_oh,h0,w1,w2,b1,b2 = arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch == 0:
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2]
      
    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      
        
    for i in range(n_batches):

      batch_indices = range(i*batch_size,(i+1)*batch_size)
      
      #partial updates
      v_w1=gamma*prev_v_w1
      v_b1=gamma*prev_v_b1
      v_w2=gamma*prev_v_w2
      v_b2=gamma*prev_v_b2
      v=[v_w1,v_b1,v_w2,v_b2]
  
      y_oh = y_train_oh[:,batch_indices].copy()
      h_0 = np.array(h0.iloc[:,batch_indices].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
      
      #Forward prop train
      a1,a2,h_1,y = forward_prop(n_hidden,activation, h_0,w1,w2,b1,b2)

      grad_a_2 = grad_softmax(y, y_oh,loss)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2-v_w2, grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1,activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v_w1+lr*grad_w_1
      v_b1=gamma*prev_v_b1+lr*grad_b_1
      v_w2=gamma*prev_v_w2+lr*grad_w_2
      v_b2=gamma*prev_v_b2+lr*grad_b_2
      
      v=[v_w1,v_b1,v_w2,v_b2]
      
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2

      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2]
      
      if i%100 == 0:
        #Forward prop train
        a_1,a_2,h1,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,b1,b2)
        #Forward prop val
        a_1_val,a_2_val,h1_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,b1,b2)
        
        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
    return w1,w2,b1,b2,v,prev_v
  
  
  if n_hidden == 2:
    y_train_oh,h0,w1,w2,w3,b1,b2,b3= arg 
    
    if epoch==0:
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3]

    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]      
    
    n_batches = int(h0.shape[1]/batch_size)
    
    for i in range(n_batches):
      
      #partial updates
      v_w1=gamma*prev_v_w1
      v_b1=gamma*prev_v_b1
      v_w2=gamma*prev_v_w2
      v_b2=gamma*prev_v_b2
      v_w3=gamma*prev_v_w3
      v_b3=gamma*prev_v_b3
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3]
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)
          
      a1,a2,a3,h_1,h_2,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,b1,b2,b3)

      grad_a_3 = grad_softmax(y, y_oh,loss)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3-v[4], grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2,activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2-v[2], grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1,activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v[0]+lr*grad_w_1
      v_b1=gamma*prev_v[1]+lr*grad_b_1
      v_w2=gamma*prev_v[2]+lr*grad_w_2
      v_b2=gamma*prev_v[3]+lr*grad_b_2
      v_w3=gamma*prev_v[4]+lr*grad_w_3
      v_b3=gamma*prev_v[5]+lr*grad_b_3
      
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3]
      
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      
      if i%100 == 0:
        
        #Forward prop
        a_1,a_2,a_3,h1,h2,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,b1,b2,b3)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,h1_val,h2_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,b1,b2,b3)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3]
     
    return w1,w2,w3,b1,b2,b3, v,prev_v
  
  if n_hidden == 3:
    y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4= arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
      prev_v_w4=np.zeros(w4.shape)
      prev_v_b4=np.zeros(b4.shape)

      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4]

    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]
      prev_v_w4=prev_v[6]
      prev_v_b4=prev_v[7]

    
    for i in range(n_batches):
      
      #partial updates
      v_w1=gamma*prev_v_w1
      v_b1=gamma*prev_v_b1
      v_w2=gamma*prev_v_w2
      v_b2=gamma*prev_v_b2
      v_w3=gamma*prev_v_w3
      v_b3=gamma*prev_v_b3
      v_w4=gamma*prev_v_w4
      v_b4=gamma*prev_v_b4
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4]
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())
      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      a1,a2,a3,a4,h_1,h_2,h_3,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,b1,b2,b3,b4)

      
      
      grad_a_4 = grad_softmax(y, y_oh,loss)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4-v[6], grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3,activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3-v[4], grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2,activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2-v[2], grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1,activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v[0]+lr*grad_w_1
      v_b1=gamma*prev_v[1]+lr*grad_b_1
      v_w2=gamma*prev_v[2]+lr*grad_w_2
      v_b2=gamma*prev_v[3]+lr*grad_b_2
      v_w3=gamma*prev_v[4]+lr*grad_w_3
      v_b3=gamma*prev_v[5]+lr*grad_b_3
      v_w4=gamma*prev_v[6]+lr*grad_w_4
      v_b4=gamma*prev_v[7]+lr*grad_b_4
      
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4]
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      w4=w4-v_w4
      b4=b4-v_b4
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      prev_v_w4=v_w4
      prev_v_b4=v_b4
      
      if i%100 == 0:
        
        #Forward prop
        a_1,a_2,a_3,a_4,h1,h2,h3,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,b1,b2,b3,b4)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,h1_val,h2_val,h3_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,b1,b2,b3,b4)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4]
     
    return w1,w2,w3,w4,b1,b2,b3,b4, v,prev_v
  
  
  if n_hidden == 4:
    
    y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5= arg 
    
    n_batches = int(h0.shape[1]/batch_size)
    
    if epoch==0:
      prev_v_w1=np.zeros(w1.shape)
      prev_v_b1=np.zeros(b1.shape)
      prev_v_w2=np.zeros(w2.shape)
      prev_v_b2=np.zeros(b2.shape)
      prev_v_w3=np.zeros(w3.shape)
      prev_v_b3=np.zeros(b3.shape)
      prev_v_w4=np.zeros(w4.shape)
      prev_v_b4=np.zeros(b4.shape)
      prev_v_w5=np.zeros(w5.shape)
      prev_v_b5=np.zeros(b5.shape)

      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4,prev_v_w5,prev_v_b5]

    else:
      prev_v_w1=prev_v[0]
      prev_v_b1=prev_v[1]
      prev_v_w2=prev_v[2]
      prev_v_b2=prev_v[3]
      prev_v_w3=prev_v[4]
      prev_v_b3=prev_v[5]
      prev_v_w4=prev_v[6]
      prev_v_b4=prev_v[7]
      prev_v_w5=prev_v[8]
      prev_v_b5=prev_v[9]


    
    for i in range(n_batches):
      
      #partial updates
      v_w1=gamma*prev_v_w1
      v_b1=gamma*prev_v_b1
      v_w2=gamma*prev_v_w2
      v_b2=gamma*prev_v_b2
      v_w3=gamma*prev_v_w3
      v_b3=gamma*prev_v_b3
      v_w4=gamma*prev_v_w4
      v_b4=gamma*prev_v_b4
      v_w5=gamma*prev_v_w5
      v_b5=gamma*prev_v_b5
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4,v_w5,v_b5]
      
      y_oh = y_train_oh[:,i*batch_size:(i+1)*batch_size].copy()
      h_0 = np.array(h0.iloc[:,i*batch_size:(i+1)*batch_size].copy())

      y_oh =  y_oh.reshape(y_oh.shape[0],-1)
      h_0 = h_0.reshape(h_0.shape[0],-1)

      #Forward prop
      a1,a2,a3,a4,a5,h_1,h_2,h_3,h_4,y = forward_prop(n_hidden,activation, h_0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

      
      grad_a_5 = grad_softmax(y, y_oh,loss)
      grad_w_5 = grad_w_k(grad_a_5, h_4)
      grad_b_5 = grad_b_k(grad_a_5)
      grad_h_4 = grad_h_i(w5-v[8], grad_a_5)
      grad_a_4 = grad_a_i(grad_h_4, a4,activation)
      grad_w_4 = grad_w_k(grad_a_4, h_3)
      grad_b_4 = grad_b_k(grad_a_4)
      grad_h_3 = grad_h_i(w4-v[6], grad_a_4)
      grad_a_3 = grad_a_i(grad_h_3, a3,activation)
      grad_w_3 = grad_w_k(grad_a_3, h_2)
      grad_b_3 = grad_b_k(grad_a_3)
      grad_h_2 = grad_h_i(w3-v[4], grad_a_3)
      grad_a_2 = grad_a_i(grad_h_2, a2,activation)
      grad_w_2 = grad_w_k(grad_a_2, h_1)
      grad_b_2 = grad_b_k(grad_a_2)
      grad_h_1 = grad_h_i(w2-v[2], grad_a_2)
      grad_a_1 = grad_a_i(grad_h_1, a1,activation)
      grad_w_1 = grad_w_k(grad_a_1, h_0)
      grad_b_1 = grad_b_k(grad_a_1)
      
      #momentum update
      v_w1=gamma*prev_v[0]+lr*grad_w_1
      v_b1=gamma*prev_v[1]+lr*grad_b_1
      v_w2=gamma*prev_v[2]+lr*grad_w_2
      v_b2=gamma*prev_v[3]+lr*grad_b_2
      v_w3=gamma*prev_v[4]+lr*grad_w_3
      v_b3=gamma*prev_v[5]+lr*grad_b_3
      v_w4=gamma*prev_v[6]+lr*grad_w_4
      v_b4=gamma*prev_v[7]+lr*grad_b_4
      v_w5=gamma*prev_v[8]+lr*grad_w_5
      v_b5=gamma*prev_v[9]+lr*grad_b_5
      
      v=[v_w1,v_b1,v_w2,v_b2,v_w3,v_b3,v_w4,v_b4,v_w5,v_b5]
            
      #update parameters
      w1=w1-v_w1
      b1=b1-v_b1
      w2=w2-v_w2
      b2=b2-v_b2
      w3=w3-v_w3
      b3=b3-v_b3
      w4=w4-v_w4
      b4=b4-v_b4
      w5=w5-v_w5
      b5=b5-v_b5
      #momentum reset
      prev_v_w1=v_w1
      prev_v_b1=v_b1
      prev_v_w2=v_w2
      prev_v_b2=v_b2
      prev_v_w3=v_w3
      prev_v_b3=v_b3
      prev_v_w4=v_w4
      prev_v_b4=v_b4
      prev_v_w5=v_w5
      prev_v_b5=v_b5
      
      if i%100 == 0:
        #Forward prop
        a_1,a_2,a_3,a_4,a_5,h1,h2,h3,h4,y_hat = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
        #Forward prop val
        a_1_val,a_2_val,a_3_val,a_4_val,a_5_val,h1_val,h2_val,h3_val,h4_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

        #Accuracy val
        y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
        val_acc = (y_val == y_pred_val).sum() / (len(y_val))

        #Loss computing
        loss_new, delta_loss = compute_loss(i, loss, y_train_oh, y_hat)
        loss_new_val, delta_loss_val = compute_loss(i, loss, y_val_oh, y_hat_val)

        #Accuracy computing
        acc = accuracy(y_train_oh, y_hat)
        err = 1- acc

        #Printing
        print(epoch,'\t\t',i,'\t\t','%.2f' % loss_new,'\t\t','%.4f' % acc,'\t\t','%.4f' % val_acc)
        log_list.append([epoch, i, loss_new, loss_new_val,err, lr])
      
      prev_v=[prev_v_w1,prev_v_b1,prev_v_w2,prev_v_b2,prev_v_w3,prev_v_b3,prev_v_w4,prev_v_b4,prev_v_w5,prev_v_b5]
     
    return w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,v, prev_v

"""## Initialising weights"""

def weights_init(n_hidden,h0, *arg):
  
  size_factor = 1
  
  if n_hidden == 1 :
    n1, n_classes = arg
    
    w1 = size_factor*np.random.randn(h0.shape[0],n1)/np.sqrt(h0.shape[0])
    b1 = size_factor*np.random.randn(n1)/np.sqrt(h0.shape[0])
    w2 = size_factor*np.random.randn(n1,n_classes)/np.sqrt(n1)
    b2 = size_factor*np.random.randn(n_classes)/np.sqrt(n1)
    return w1,b1,w2,b2
  
  if n_hidden == 2 :
    n1,n2, n_classes = arg
    
    w1 = size_factor*np.random.randn(h0.shape[0],n1)/np.sqrt(h0.shape[0])
    b1 = size_factor*np.random.randn(n1)/np.sqrt(h0.shape[0])
    w2 = size_factor*np.random.randn(n1,n2)/np.sqrt(n1)
    b2 = size_factor*np.random.randn(n2)/np.sqrt(n1)
    w3 = size_factor*np.random.randn(n2,n_classes)/np.sqrt(n2)
    b3 = size_factor*np.random.randn(n_classes)/np.sqrt(n2)
    return w1,b1,w2,b2,w3,b3
    
  if n_hidden == 3 :
    n1,n2,n3,n_classes = arg
    
    w1 = size_factor*np.random.randn(h0.shape[0],n1)/np.sqrt(h0.shape[0])
    b1 = size_factor*np.random.randn(n1)/np.sqrt(h0.shape[0])
    w2 = size_factor*np.random.randn(n1,n2)/np.sqrt(n1)
    b2 = size_factor*np.random.randn(n2)/np.sqrt(n1)
    w3 = size_factor*np.random.randn(n2,n3)/np.sqrt(n2)
    b3 = size_factor*np.random.randn(n3)/np.sqrt(n2)
    w4 = size_factor*np.random.randn(n3,n_classes)/np.sqrt(n3)
    b4 = size_factor*np.random.randn(n_classes)/np.sqrt(n3)
    return w1,b1,w2,b2,w3,b3,w4,b4
    
  if n_hidden == 4 :
    n1,n2,n3,n4, n_classes = arg
    
    w1 = size_factor*np.random.randn(h0.shape[0],n1)/np.sqrt(h0.shape[0])
    b1 = size_factor*np.random.randn(n1)/np.sqrt(h0.shape[0])
    w2 = size_factor*np.random.randn(n1,n2)/np.sqrt(n1)
    b2 = size_factor*np.random.randn(n2)/np.sqrt(n1)
    w3 = size_factor*np.random.randn(n2,n3)/np.sqrt(n2)
    b3 = size_factor*np.random.randn(n3)/np.sqrt(n2)
    w4 = size_factor*np.random.randn(n3,n4)/np.sqrt(n3)
    b4 = size_factor*np.random.randn(n4)/np.sqrt(n3)
    w5 = size_factor*np.random.randn(n4,n_classes)/np.sqrt(n4)
    b5 = size_factor*np.random.randn(n_classes)/np.sqrt(n4)
    return w1,b1,w2,b2,w3,b3,w4,b4,w5,b5

"""## Master function"""

def ann(n_hidden,sizes, alpha, n_epochs, activ, loss_function, anneal,batch_size, optimizer = 'gd', gamma = 0.9, weight_init_ = True, list_of_weights_loaded=[]):
  
  activation = activation0(activ)
  
  print('Epoch\t\t','Step\t\t','Loss', '\t\t', 'Train accuracy','\t','Validation accuracy')
  
  if n_hidden == 1:
    n1 = sizes[0]
    if weight_init_ == True:
      w1,b1,w2,b2 = weights_init(1,h0, n1, 10)
    else:
      w1,b1,w2,b2 = unpack_weights(list_of_weights_loaded)
    delta_loss = 10000
    loss_new = 10**10
    loss_new_val = 10**10
    log_list = []
    prev_v = []; m = [] ; v = []
    i = 0

    while(i<n_epochs):
            
      #Forward prop train
      a_1_e,a_2_e,h1_e,y_hat_e = forward_prop(n_hidden,activation, h0,w1,w2,b1,b2)
      #Forward prop val
      a_1_val,a_2_val,h1_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,b1,b2)

      #Accuracy val
      y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
      val_acc = (y_val == y_pred_val).sum() / (len(y_val))

      #Loss computing
      loss_old = loss_new
      loss_new, delta_loss = compute_loss(i, loss_function, y_train_oh, y_hat_e, loss_old)
      loss_old_val = loss_new_val
      loss_new_val, delta_loss_val = compute_loss(i, loss_function, y_val_oh, y_hat_val, loss_old_val)
      
      #Anneal
      if anneal:
        if delta_loss_val > 0:
          alpha = alpha/2
          w1 = old_w[0];b1 = old_w[1];w2=old_w[2];b2=old_w[3]
          print('annealed to alpha = {}'.format(alpha) )

      #Accuracy computing
      acc = accuracy(y_train_oh, y_hat_e)
      err = 1- acc
      
      #Saving old weights for annealing
      old_w = [w1,b1,w2,b2]
      
      #Backward prop
      if optimizer == 'gd':
        w1,w2,b1,b2 = back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,y_train_oh,h0,w1,w2,b1,b2)
      elif optimizer == 'momentum':
        w1,b1,w2,b2,prev_v = momentum_back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,gamma,y_train_oh,h0,w1,w2,b1,b2,prev_v)
      elif optimizer == 'adam':
        w1,w2,b1,b2,m,v = back_prop_batch_ADAM(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,0.9,0.999,1e-8,m,v,y_train_oh,h0,w1,w2,b1,b2)
      elif optimizer == 'nag':
        w1,w2,b1,b2,v,prev_v = back_prop_batch_NAG(log_list,i,n_hidden,alpha,batch_size,activation,loss_function,gamma,v,prev_v,y_train_oh,h0,w1,w2,b1,b2)
      
      list_of_weights=[w1,w2,b1,b2]
      save_weights(list_of_weights,i,save_dir)
      
      i = i+1  
        
    return w1,b1,w2,b2,log_list

  
  if n_hidden == 2:
    
    n1 = sizes[0]
    n2 = sizes[1]
    if weight_init_ == True:
      w1,b1,w2,b2,w3,b3 = weights_init(2,h0, n1, n2, 10)
    else:
      w1,b1,w2,b2,w3,b3 = unpack_weights(list_of_weights_loaded)
    delta_loss = 10000
    loss_new = 10**10
    loss_new_val = 10**10
    log_list = []
    prev_v = [] ; m=[] ; v = []
    i = 0

    while(i<n_epochs):

      #Forward prop
      a_1_e,a_2_e,a_3_e,h1_e,h2_e,y_hat_e = forward_prop(n_hidden,activation, h0,w1,w2,w3,b1,b2,b3)
      #Forward prop val
      a_1_val,a_2_val,a_3_val,h1_val,h2_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,b1,b2,b3)

      #Accuracy val
      y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
      val_acc = (y_val == y_pred_val).sum() / (len(y_val))

      #Loss computing
      loss_old = loss_new
      loss_new, delta_loss = compute_loss(i, loss_function, y_train_oh, y_hat_e, loss_old)
      loss_old_val = loss_new_val
      loss_new_val, delta_loss_val = compute_loss(i, loss_function, y_val_oh, y_hat_val,loss_old_val)
      
      #Anneal
      if anneal:
        if delta_loss_val > 0:
          alpha = alpha/2
          w1 = old_w[0];b1 = old_w[1];w2=old_w[2];b2=old_w[3];w3=old_w[4];b3=old_w[5]
          print('annealed to alpha = {}'.format(alpha) )

      #Accuracy computing
      acc = accuracy(y_train_oh, y_hat_e)
      err = 1- acc
      
      #Saving old weights for annealing
      old_w = [w1,b1,w2,b2,w3,b3]

      #Backward prop
      if optimizer == 'gd':
        w1,w2,w3,b1,b2,b3 = back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function, y_train_oh,h0,w1,w2,w3,b1,b2,b3)
      elif optimizer == 'momentum':
        w1,b1,w2,b2,w3,b3,prev_v = momentum_back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,gamma,y_train_oh,h0,w1,w2,w3,b1,b2,b3,prev_v)
      elif optimizer == 'adam':
        w1,w2,w3,b1,b2,b3,m,v = back_prop_batch_ADAM(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,0.9,0.999,1e-8,m,v,y_train_oh,h0,w1,w2,w3,b1,b2,b3)
      elif optimizer == 'nag':
        w1,w2,w3,b1,b2,b3,v,prev_v = back_prop_batch_NAG(log_list,i,n_hidden,alpha,batch_size,activation,loss_function,gamma,v,prev_v,y_train_oh,h0,w1,w2,w3,b1,b2,b3) 
        
      list_of_weights=[w1,w2,w3,b1,b2,b3]
      save_weights(list_of_weights,i,save_dir)
  
      i = i+1
          
    return w1,b1,w2,b2,w3,b3,log_list
  
  
  if n_hidden == 3:
    
    n1 = sizes[0]
    n2 = sizes[1]
    n3 = sizes[2]
    if weight_init_ == True:
      w1,b1,w2,b2,w3,b3,w4,b4 = weights_init(3,h0, n1, n2, n3, 10)
    else:
      w1,b1,w2,b2,w3,b3,w4,b4 = unpack_weights(list_of_weights_loaded)
    delta_loss = 10000
    loss_new = 10**10
    loss_new_val = 10**10
    log_list = []
    prev_v = []; m = []; v = []
    i = 0

    while(i<n_epochs):

      #Forward prop
      a_1_e,a_2_e,a_3_e,a_4_e,h1_e,h2_e,h3_e,y_hat_e = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,b1,b2,b3,b4)
      #Forward prop val
      a_1_val,a_2_val,a_3_val,a_4_val,h1_val,h2_val,h3_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,b1,b2,b3,b4)

      #Accuracy val
      y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
      val_acc = (y_val == y_pred_val).sum() / (len(y_val))

      #Loss computing
      loss_old = loss_new
      loss_new, delta_loss = compute_loss(i, loss_function, y_train_oh, y_hat_e, loss_old)
      loss_old_val = loss_new_val
      loss_new_val, delta_loss_val = compute_loss(i, loss_function, y_val_oh, y_hat_val,loss_old_val)
      
      #Anneal
      if anneal:
        if delta_loss_val > 0:
          alpha = alpha/2
          w1 = old_w[0];b1 = old_w[1];w2=old_w[2];b2=old_w[3];w3=old_w[4];b3=old_w[5];w4=old_w[6];b4=old_w[7]
          print('annealed to alpha = {}'.format(alpha) )

      #Accuracy computing
      acc = accuracy(y_train_oh, y_hat_e)
      err = 1- acc
      
      #Saving old weights for annealing
      old_w = [w1,b1,w2,b2,w3,b3,w4,b4]

      #Backward prop
      if optimizer == 'gd':
        w1,w2,w3,w4,b1,b2,b3,b4 = back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function, y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4)
      elif optimizer == 'momentum':
        w1,b1,w2,b2,w3,b3,w4,b4,prev_v = momentum_back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,gamma,y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4,prev_v)
      elif optimizer == 'adam':
        w1,w2,w3,w4,b1,b2,b3,b4,m,v = back_prop_batch_ADAM(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,0.9,0.999,1e-8,m,v,y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4)
      elif optimizer == 'nag':
        w1,w2,w3,w4,b1,b2,b3,b4,v,prev_v = back_prop_batch_NAG(log_list,i,n_hidden,alpha,batch_size,activation,loss_function,gamma,v,prev_v,y_train_oh,h0,w1,w2,w3,w4,b1,b2,b3,b4) 
        
      list_of_weights=[w1,w2,w3,w4,b1,b2,b3,b4]
      save_weights(list_of_weights,i,save_dir)
      
      i = i+1
          
    return w1,b1,w2,b2,w3,b3,w4,b4,log_list
  
  
  
  if n_hidden == 4:
    
    n1 = sizes[0]
    n2 = sizes[1]
    n3 = sizes[2]
    n4 = sizes[3]
    if weight_init_ == True:
      w1,b1,w2,b2,w3,b3,w4,b4,w5,b5 = weights_init(4, h0, n1, n2, n3, n4, 10)
    else:
      w1,b1,w2,b2,w3,b3,w4,b4,w5,b5 = unpack_weights(list_of_weights_loaded)
    delta_loss = 10000
    loss_new = 10**10
    loss_new_val = 10**10
    log_list = []
    prev_v = [] ; m=[]; v = []
    i = 0

    while(i<n_epochs):

      #Forward prop
      a_1_e,a_2_e,a_3_e,a_4_e,a_5_e,h1_e,h2_e,h3_e,h4_e,y_hat_e = forward_prop(n_hidden,activation, h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
      #Forward prop val
      a_1_val,a_2_val,a_3_val,a_4_val,a_5_val,h1_val,h2_val,h3_val,h4_val,y_hat_val = forward_prop(n_hidden,activation, h0_val,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)

      #Accuracy val
      y_pred_val = np.array([ (np.argmax(y)) for y in y_hat_val.T] )
      val_acc = (y_val == y_pred_val).sum() / (len(y_val))

      #Loss computing
      loss_old = loss_new
      loss_new, delta_loss = compute_loss(i, loss_function, y_train_oh, y_hat_e, loss_old)
      loss_old_val = loss_new_val
      loss_new_val, delta_loss_val = compute_loss(i, loss_function, y_val_oh, y_hat_val,loss_old_val)
      
      #Anneal
      if anneal:
        if delta_loss_val > 0:
          alpha = alpha/2
          w1 = old_w[0];b1 = old_w[1];w2=old_w[2];b2=old_w[3];w3=old_w[4];b3=old_w[5];w4=old_w[6];b4=old_w[7];w5=old_w[8];b5=old_w[9]
          print('annealed to alpha = {}'.format(alpha) )

      #Accuracy computing
      acc = accuracy(y_train_oh, y_hat_e)
      err = 1- acc
      
      #Saving old weights for annealing
      old_w = [w1,b1,w2,b2,w3,b3,w4,b4,w5,b5]

      #Backward prop
      if optimizer == 'gd':
        w1,w2,w3,w4,w5,b1,b2,b3,b4,b5 = back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function, y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
      elif optimizer == 'momentum':
        w1,b1,w2,b2,w3,b3,w4,b4,w5,b5,prev_v = momentum_back_prop_batch(log_list,i,n_hidden, alpha,batch_size,activation,loss_function,gamma,y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,prev_v)
      elif optimizer == 'adam':
        w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,m,v = back_prop_batch_ADAM(log_list,i,n_hidden,alpha,batch_size,activation,loss_function,0.9,0.999,1e-8,m,v,y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5)
      elif optimizer == 'nag':
        w1,w2,w3,w4,w5,b1,b2,b3,b4,b5,v,prev_v = back_prop_batch_NAG(log_list,i,n_hidden,alpha,batch_size,activation,loss_function,gamma,v,prev_v,y_train_oh,h0,w1,w2,w3,w4,w5,b1,b2,b3,b4,b5) 
        
      list_of_weights=[w1,w2,w3,w4,w5,b1,b2,b3,b4,b5]
      save_weights(list_of_weights,i,save_dir)
      
      i = i+1
          
    return w1,b1,w2,b2,w3,b3,w4,b4,w5,b5,log_list

if __name__=='__main__':
  main()

